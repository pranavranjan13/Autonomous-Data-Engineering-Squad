# Infrastructure Config — Auto-generated by Cloud_Architect
# Task: Write a concise PySpark script for 500GB of shipping logs, partitioned by event_date.
================================================================================

<think>
Okay, let's see. The user provided a PySpark script and wants the deployment configurations for both AWS Glue and Azure Databricks. 

First, for the AWS Glue section. They mentioned using Terraform. So I need to create an AWS Glue job. The script uses PySpark, so the Glue job should be a Python shell job, but wait, no—AWS Glue supports PySpark natively, so it's a Spark job. Wait, AWS Glue jobs can be of type Spark. The user specified worker_type as G.2X, number_of_workers 10, and glue_version 4.0. The IAM role needs to trust glue.amazonaws.com. Also, the script should be uploaded to an S3 bucket, and the job command should reference that S3 path. 

I should create an aws_iam_role with the correct trust policy. Then an aws_iam_role_policy_attachment for AWSGlueServiceRole. The aws_glue_job will have the required parameters. The script is in S3, so the command should use the Python shell with the script's S3 path. Also, the execution role should be the IAM role created.

Now for Azure Databricks. The user wants a YAML pipeline for Azure DevOps. The pipeline should trigger on the main branch. The job runs a Databricks notebook. The cluster needs to be Standard_DS3_v2 with spark_version 13.3.x-scala2.12 and 8 workers. The notebook path would be in the Databricks workspace. The pipeline should use the Databricks CLI to submit the job. 

In the YAML, the trigger is on the main branch. The steps include using the Azure CLI to authenticate and then run the Databricks job. The service principal credentials need to be handled securely, perhaps using Azure Key Vault. The cluster configuration in Databricks should be set up with the specified VM type and Spark version.

Wait, the user provided an approved script. The script is written for AWS Glue and Databricks, but the deployment configurations are separate. The script itself doesn't need to be modified for the deployment, just referenced in the configurations. 

Possible issues: Making sure the IAM roles and permissions are correctly set up, and that the Databricks pipeline has the correct service principal access. Also, ensuring the script path is correct in both S3 and Databricks workspace.

I need to structure the two sections clearly. For AWS, the Terraform code for IAM role and Glue job. For Azure, the YAML pipeline with the correct cluster and job settings. Also, note that in Databricks, the cluster might need to be pre-configured, but the YAML can reference it by name or ID. Alternatively, create the cluster as part of the pipeline steps. But the user's instruction says to have the cluster in the YAML pipeline, so probably using a Databricks CLI command to create the cluster or reference an existing one.

Wait, the user's example for Azure Databricks YAML might just include the pipeline triggering and the cluster configuration. So in the steps, the pipeline would use the Databricks CLI to run the notebook, specifying the cluster ID or name. But how to handle the cluster in the YAML? Maybe the cluster is already set up, or the pipeline can create it. Since the user specified the cluster details, the YAML should include the necessary parameters to reference or create the cluster.

I think the YAML will have a job that runs a Databricks notebook on a specific cluster. The cluster configuration can be part of the Databricks setup, so in the YAML, the step would include the cluster name or ID. But perhaps the cluster is created separately, and the pipeline just uses it. The user's instruction says to include the cluster details in the YAML, so maybe the YAML includes the cluster configuration. Wait, no—the pipeline YAML typically triggers the job, not create the cluster. The cluster would be set up in Databricks with the specified parameters, and the pipeline uses that cluster's ID. But to make it self-contained, maybe the YAML includes steps to create the cluster using the Databricks CLI. However, that might be more complex. The user's example might just reference the cluster by name or ID in the YAML.

So in the YAML, the pipeline step would use the Databricks CLI to run the notebook, specifying the existing cluster. The cluster is already created with the specified parameters. Therefore, the YAML would have the cluster ID or name as a parameter. Since the user wants the cluster details in the YAML, perhaps the YAML includes the cluster configuration as part of the job's parameters. Alternatively, the cluster is created via Terraform or another method, and the YAML just references it. 

In any case, the key points are: the pipeline triggers on main branch, uses the specified cluster configuration, and runs the notebook. The cluster has Standard_DS3_v2 nodes, spark version 13.3.x, and 8 workers. The YAML should include these details in the steps when invoking the Databricks CLI.
</think>

### SECTION 1: AWS Glue (Terraform)  
```hcl
resource "aws_iam_role" "glue_role" {
  name = "glue_execution_role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = { Service = "glue.amazonaws.com" }
      Action = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_role_policy_attachment" "glue_service_role" {
  role       = aws_iam_role.glue_role.name
  policy_arn = "arn:aws:iam::aws:policy/AWSGlue