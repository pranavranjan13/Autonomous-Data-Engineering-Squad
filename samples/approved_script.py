# Auto-generated by Data_Architect
# Review Status: APPROVED

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType
from pyspark.sql.functions import col, to_date, count

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("FixPartitionByIssue") \
    .getOrCreate()

# [RULE 3] Define explicit schema
schema = StructType([
    StructField("event_id", StringType(), True),
    StructField("event_timestamp", TimestampType(), True),
    StructField("user_id", StringType(), True),
    StructField("value", IntegerType(), True)
])

# Sample data
data = [
    ("e1", "2023-01-01 10:00:00", "u1", 10),
    ("e2", "2023-01-01 11:00:00", "u2", 20),
    ("e3", "2023-01-02 12:00:00", "u1", 15),
    ("e4", "2023-01-02 13:00:00", "u3", 25),
    ("e5", "2023-01-03 14:00:00", "u2", 30),
    ("e6", "2023-01-03 15:00:00", "u1", 5)
]

# Create DataFrame
df = spark.createDataFrame(data, schema)

# [RULE 2] Derive event_date from event_timestamp
df_with_date = df.withColumn("event_date", to_date(col("event_timestamp")))

# Perform some sample aggregation
aggregated_df = df_with_date.groupBy("event_date", "user_id").agg(
    count("event_id").alias("event_count")
)

# [RULE 1] Save the output, partitioned by 'event_date'
# Using an example path for saving
output_path = "/tmp/processed_events"
aggregated_df.write \
    .mode("overwrite") \
    .partitionBy("event_date") \
    .parquet(output_path)

print(f"Data successfully written to {output_path} partitioned by event_date.")

# Stop SparkSession
spark.stop()